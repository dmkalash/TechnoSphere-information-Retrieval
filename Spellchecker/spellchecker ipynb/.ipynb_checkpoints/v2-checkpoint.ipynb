{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import heapq\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spell / not spell: 115232 / 1884768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_letter_dict(origin, fixed):\n",
    "    \n",
    "    table = np.zeros((len(fixed) + 1, len(origin) + 1))\n",
    "    table[0] = np.arange(len(origin) + 1)\n",
    "    table[:, 0] = np.arange(len(fixed) + 1)\n",
    "    \n",
    "    for i in range( 1, len(fixed) + 1 ):\n",
    "        for j in range( 1, len(origin) + 1 ):\n",
    "            #print(len(origin), j - 1, len(fixed), i - 1, (origin[j - 1] != fixed[i - 1]))\n",
    "            table[i, j] = min( table[i - 1, j] + 1,\n",
    "                                table[i, j - 1] + 1,\n",
    "                                table[i - 1, j - 1] + (origin[j - 1] != fixed[i - 1]) )\n",
    "            \n",
    "    part_letter_dict = dict()\n",
    "    #one_letter_dict = dict()\n",
    "    \n",
    "    i, j = len(fixed), len(origin)\n",
    "    while i != 0 and j != 0:\n",
    "#         m = min( table[i - 1, j] + 1,\n",
    "#                     table[i, j - 1] + 1,\n",
    "#                     table[i - 1, j - 1] + (origin[j - 1] != fixed[i - 1]) )\n",
    "        if table[i - 1, j - 1] + (origin[j - 1] != fixed[i - 1]) == table[i, j]:\n",
    "            symb = origin[j - 1] + fixed[i - 1]\n",
    "            part_letter_dict[symb] = part_letter_dict.get(symb, 0) + 1\n",
    "            #one_letter_dict[origin[j - 1]] = one_letter_dict.get(origin[j - 1], 0) + 1\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "            \n",
    "        elif table[i - 1, j] + 1 == table[i, j]:\n",
    "            symb = '_' + fixed[i - 1]\n",
    "            part_letter_dict[symb] = part_letter_dict.get(symb, 0) + 1\n",
    "            #one_letter_dict[origin[j - 1]] = one_letter_dict.get(origin[j - 1], 0) + 1\n",
    "            i -= 1\n",
    "        else:\n",
    "            symb = origin[j - 1] + '_'\n",
    "            part_letter_dict[symb] = part_letter_dict.get(symb, 0) + 1\n",
    "            #one_letter_dict['_'] = one_letter_dict.get('_', 0) + 1\n",
    "            j -= 1\n",
    "    \n",
    "    while i != 0:\n",
    "        symb = '_' + fixed[i - 1]\n",
    "        part_letter_dict[symb] = part_letter_dict.get(symb, 0) + 1\n",
    "        #one_letter_dict[origin[j - 1]] = one_letter_dict.get(origin[j - 1], 0) + 1\n",
    "        i -= 1\n",
    "        \n",
    "    while j != 0:\n",
    "        symb = origin[j - 1] + '_'\n",
    "        part_letter_dict[symb] = part_letter_dict.get(symb, 0) + 1\n",
    "        #one_letter_dict['_'] = one_letter_dict.get('_', 0) + 1\n",
    "        j -= 1\n",
    "\n",
    "    return part_letter_dict#, one_letter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_query(query):\n",
    "    tokens = re.findall(r'\\w+', query)\n",
    "    #tokens = re.findall(r'[A-Za-zа-яА-Я]+', pair[0])\n",
    "            \n",
    "    tokens = list( map(lambda s: s.lower(), tokens))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_prior_dict(fname='queries_all.txt'):\n",
    "    with open(fname) as fin:\n",
    "        words_dict = dict()\n",
    "        bigramm_words_dict = dict()\n",
    "        letter_dict = dict()\n",
    "        #one_letter_dict = dict()\n",
    "        hard_queries = []\n",
    "        \n",
    "        for i, line in tqdm(enumerate(fin)):\n",
    "            pair = line.split('\\t')\n",
    "            if len(pair) > 1:\n",
    "                pair[0], pair[1] = pair[1], pair[0]\n",
    "            else:\n",
    "                pair.append(pair[0])\n",
    "                \n",
    "            tokens = prepare_query( pair[0] )\n",
    "            alpha_tokens = list(filter(lambda s: s.isalpha(), tokens))\n",
    "\n",
    "            for token in set(alpha_tokens):\n",
    "                words_dict[token] = words_dict.get(token, 0) + 1\n",
    "\n",
    "            for w12 in zip(alpha_tokens, alpha_tokens[1:]):\n",
    "                key = '_'.join(w12)\n",
    "                bigramm_words_dict[key] = bigramm_words_dict.get(key, 0) + 1\n",
    "                \n",
    "            if len(pair) > 1:\n",
    "                origin_tokens = prepare_query( pair[1] )\n",
    "                if len(origin_tokens) != len(tokens):\n",
    "                    hard_queries.append((i, line))\n",
    "                    #print(i, origin_tokens, tokens, line)\n",
    "                    continue\n",
    "                \n",
    "                for origin, fixed in zip(origin_tokens, tokens):\n",
    "                    part_letter_dict = get_part_letter_dict(origin, fixed) # , part_one_letter_dict\n",
    "                    for key in part_letter_dict:\n",
    "                        letter_dict[key] = letter_dict.get(key, 0) + 1\n",
    "#                     for key in one_letter_dict:\n",
    "#                         one_letter_dict[key] = one_letter_dict.get(key, 0) + 1\n",
    "                \n",
    "            if i > 3000:\n",
    "                break\n",
    "\n",
    "        words_prior_dict = dict()\n",
    "        for key in words_dict:\n",
    "            words_prior_dict[key] = words_dict[key] / len(words_dict)\n",
    "            \n",
    "        for key in letter_dict:\n",
    "            letter_dict[key] = letter_dict[key] / len(letter_dict) # one_letter_dict[key[0]]\n",
    "            \n",
    "        # smoothing mb:\n",
    "        # words_prior_dict[key] = (words_prior_dict[key] + 1) / ( 2 * len(words_prior_dict))\n",
    "\n",
    "        return words_dict, words_prior_dict, bigramm_words_dict, letter_dict, hard_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4f0b3dadf34cb592106e75cb376d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "words_dict, words_prior_dict, bigramm_words_dict, letter_dict, hard_queries = get_words_prior_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('words_dict.pkl', 'rb') as output:\n",
    "    words_dict = pickle.load(output)\n",
    "    \n",
    "with open('bigramm_words_dict.pkl', 'rb') as output:\n",
    "    bigramm_words_dict = pickle.load(output)\n",
    "    \n",
    "with open('words_prior_dict.pkl', 'rb') as output:\n",
    "    words_prior_dict = pickle.load(output)\n",
    "    \n",
    "with open('letter_dict.pkl', 'rb') as output:\n",
    "    letter_dict = pickle.load(output)\n",
    "\n",
    "with open('hard_queries.pkl', 'rb') as output:\n",
    "    hard_queries = pickle.load(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pickle\n",
    "# with open('words_dict.pkl', 'wb') as output:\n",
    "#     pickle.dump(words_dict, output, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('bigramm_words_dict.pkl', 'wb') as output:\n",
    "#     pickle.dump(bigramm_words_dict, output, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('words_prior_dict.pkl', 'wb') as output:\n",
    "#     pickle.dump(words_prior_dict, output, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('letter_dict.pkl', 'wb') as output:\n",
    "#     pickle.dump(letter_dict, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('hard_queries.pkl', 'wb') as output:\n",
    "#     pickle.dump(hard_queries, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm = sum(letter_dict.values())\n",
    "for key in letter_dict:\n",
    "    letter_dict[key] = letter_dict[key] / sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Levenstein:\n",
    "    def __init__(self, weights):\n",
    "        self.weights = weights.copy()\n",
    "        for key in self.weights:\n",
    "            self.weights[key] = -math.log(self.weights[key])\n",
    "        self.eps = max(self.weights.values())\n",
    "        #self.eps = min(self.weights.values())\n",
    "    \n",
    "    def get_distance(self, origin, fixed, alpha=1.1, coef=1): # 10 TODO: вероятность \n",
    "        table = np.zeros((len(fixed) + 1, len(origin) + 1))\n",
    "        \n",
    "        for j in range(1, len(origin) + 1):\n",
    "            table[0][j] = table[0, j - 1] + self.weights.get(origin[j - 1] + '_', 0)\n",
    "\n",
    "        for i in range(1, len(fixed) + 1):\n",
    "            table[i][0] = table[i - 1, 0] + self.weights.get('_' + fixed[i - 1], 0)\n",
    "        \n",
    "        #table[0] = #np.arange(len(origin) + 1)\n",
    "        #table[:, 0] = #np.arange(len(fixed) + 1)\n",
    "        \n",
    "        for i in range( 1, len(fixed) + 1 ):\n",
    "            for j in range( 1, len(origin) + 1 ):\n",
    "                table[i, j] = min( table[i - 1, j] + self.weights.get('_' + fixed[i - 1], self.eps),\n",
    "                                    table[i, j - 1] + self.weights.get(origin[j - 1] + '_', self.eps),\n",
    "                                    table[i - 1, j - 1] + (origin[j - 1] != fixed[i - 1]) * \n",
    "                                          self.weights.get(origin[j - 1] + fixed[i - 1], self.eps) )\n",
    "                #print(origin[j - 1], fixed[i - 1], self.weights.get(origin[j - 1] + fixed[i - 1]) )\n",
    "                \n",
    "#                 table[i, j] = min( table[i - 1, j] + ( 1 - coef * self.weights.get('_' + fixed[i - 1], 0) ) ,\n",
    "#                                     table[i, j - 1] + ( 1 - coef * self.weights.get(origin[j - 1] + '_', 0) ),\n",
    "#                                     table[i - 1, j - 1] + (origin[j - 1] != fixed[i - 1]) * \n",
    "#                                                   ( 1 - coef * self.weights.get(origin[j - 1] + fixed[i - 1], 0)) )\n",
    "        #print(table)\n",
    "        #print('dist:', table[-1, -1])\n",
    "        return alpha ** (-coef * table[-1, -1])\n",
    "        #return 1 / 1 + math.exp(coef * table[-1, -1])\n",
    "        \n",
    "\n",
    "class Levenstein1:\n",
    "    def __init__(self, weights=None):\n",
    "        pass\n",
    "    \n",
    "    def get_distance(self, origin, fixed, alpha=2.7, coef=1): # 10 TODO: вероятность \n",
    "        table = np.zeros((len(fixed) + 1, len(origin) + 1))  \n",
    "        table[0] = np.arange(len(origin) + 1)\n",
    "        table[:, 0] = np.arange(len(fixed) + 1)\n",
    "        \n",
    "        for i in range( 1, len(fixed) + 1 ):\n",
    "            for j in range( 1, len(origin) + 1 ):\n",
    "                table[i, j] = min( table[i - 1, j] + 1,\n",
    "                                    table[i, j - 1] + 1,\n",
    "                                    table[i - 1, j - 1] + (origin[j - 1] != fixed[i - 1]) )\n",
    "            \n",
    "        #print('dist:', table[-1, -1])\n",
    "        return alpha ** (-coef * table[-1, -1])\n",
    "        #return 1 / 1 + math.exp(coef * table[-1, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18926797441218643\n",
      "1.0\n",
      "0.05597063733469807\n",
      "0.009050801663135058\n",
      "0.4432197591749109\n",
      "0.004708167461186156\n",
      "CPU times: user 7.32 ms, sys: 19 ms, total: 26.4 ms\n",
      "Wall time: 52 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lev = Levenstein(letter_dict)\n",
    "print( lev.get_distance('еееm', 'ееее') )\n",
    "print( lev.get_distance('ееее', 'ееее') )\n",
    "print( lev.get_distance('абвг', 'аапг') )\n",
    "print( lev.get_distance('аааа', 'вввв') )\n",
    "print( lev.get_distance('праспект', 'проспект') )\n",
    "print( lev.get_distance('праспект', 'прав') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, num, freq=0, is_term=False):\n",
    "        self.num = num\n",
    "        self.children = dict() # symb : num\n",
    "        self.freq = freq\n",
    "        self.is_term = is_term\n",
    "\n",
    "class Trie: \n",
    "    def __init__(self):\n",
    "        self.data = {0 : Node(0, 0, True)} \n",
    "        \n",
    "    def build_tree(self, words):\n",
    "        #words.sort()\n",
    "        for word in words:\n",
    "            cur_node_num = 0\n",
    "            \n",
    "            for symb in word:    \n",
    "                cur_node = self.data[cur_node_num]\n",
    "                cur_node.freq += 1\n",
    "                if symb not in cur_node.children:\n",
    "                    new_num = len(self.data)\n",
    "                    self.data[new_num] = Node(new_num)\n",
    "                    cur_node.children[symb] = new_num\n",
    "                    cur_node_num = new_num\n",
    "                    \n",
    "                cur_node_num = cur_node.children[symb]\n",
    "            \n",
    "            self.data[cur_node_num].is_term = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.73 s, sys: 2.36 s, total: 9.09 s\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trie = Trie()\n",
    "trie.build_tree(list(words_dict.keys())) # [:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Min Heap Implementation in Python\n",
    "\"\"\"\n",
    "class MinHeap:\n",
    "    def __init__(self, max_heap_size, init=[-1e8, -1, -1,'']):\n",
    "        \"\"\"\n",
    "        On this implementation the heap list is initialized with a value\n",
    "        \"\"\"\n",
    "        self.init = init\n",
    "        self.heap_list = [init] + [None] * max_heap_size # cur_weight, node_num, origin_i, preffix\n",
    "        self.current_size = 0\n",
    "        self.map = dict() #{(init[2], init[3]) : 0} # (origin_i, preffix): heap_node_num\n",
    "        self.max_heap_size = max_heap_size\n",
    "\n",
    "    def merge(self, heap_b): # TODO: new_heap можно через обычный список делать и индексировать с 0\n",
    "        for elem in heap_b.heap_list[1 : heap_b.current_size + 1]:\n",
    "            self.insert(elem)\n",
    " \n",
    "    def sift_up(self, i):\n",
    "        \"\"\"\n",
    "        Moves the value up in the tree to maintain the heap property.\n",
    "        \"\"\"\n",
    "        #print('sift_up')\n",
    "        while i // 2 > 0:\n",
    "            if self.heap_list[i][0] < self.heap_list[i // 2][0]:\n",
    "                self.map_swap(i, i // 2)\n",
    "                self.heap_list[i], self.heap_list[i // 2] = self.heap_list[i // 2], self.heap_list[i]\n",
    "            i = i // 2\n",
    " \n",
    "\n",
    "    def insert(self, k):\n",
    "        \"\"\"\n",
    "        Inserts a value into the heap\n",
    "        \"\"\"\n",
    "        #print(my_heap.heap_list, self.map.get((k[2], k[1]), None))\n",
    " \n",
    "        heap_node_num = self.map.get((k[2], k[3]), None)\n",
    "        if heap_node_num is None:\n",
    "            self.current_size += 1\n",
    "            self.heap_list[self.current_size] = k\n",
    "            self.map[(k[2], k[3])] = self.current_size\n",
    "            \n",
    "            self.sift_up(self.current_size)\n",
    "            if self.current_size >= self.max_heap_size:\n",
    "                # TODO: может быть по порогу лучше отсекать(правда мб будет 0 вариантов), тк \n",
    "                # сейчас мы не максимальный штраф удаляем, а рандомный в конце списка\n",
    "                \n",
    "                #print('DELETE!')\n",
    "                #print( len(self.map, len(self.heap_list)) )\n",
    "                del self.map[ (self.heap_list[self.current_size][2], self.heap_list[self.current_size][3]) ] # TODO: change\n",
    "                #del self.heap_list[-1]\n",
    "                self.current_size -= 1\n",
    "                #print( len(self.map, len(self.heap_list)) )\n",
    "                #print()\n",
    "\n",
    "\n",
    "        elif k[0] < self.heap_list[heap_node_num][0]:\n",
    "            self.heap_list[heap_node_num] = k\n",
    "            self.sift_up(heap_node_num)\n",
    "\n",
    "#        try:\n",
    "\n",
    "#             else:\n",
    "#                 print(k[0], self.heap_list[heap_node_num][0], heap_node_num)\n",
    "                \n",
    "#         except:\n",
    "#             print()\n",
    "#             print('k:',k)\n",
    "#             print('heap_node_num:', heap_node_num)\n",
    "#             print('self.heap_list:', self.heap_list )\n",
    "#             print(self.current_size)\n",
    "#             print(self.map)\n",
    "#             print( len(self.map))\n",
    "#             raise\n",
    "        \n",
    "#         t = [(a, b) for _, _, a, b in self.heap_list]\n",
    "#         if len(t) != len(set(t)):\n",
    "#             print('list:', self.heap_list)\n",
    "        \n",
    " \n",
    "    def sift_down(self, i):\n",
    "        #print('sift_down')\n",
    "        while i * 2 <= self.current_size:\n",
    "            mc = self.min_child(i)\n",
    "            if self.heap_list[i][0] > self.heap_list[mc][0]:\n",
    "                self.map_swap(i, mc)\n",
    "                self.heap_list[i], self.heap_list[mc] = self.heap_list[mc], self.heap_list[i]\n",
    "                i = mc\n",
    "            else:\n",
    "                break\n",
    " \n",
    "    def min_child(self, i):\n",
    "        if i * 2 + 1 > self.current_size:\n",
    "            return i * 2\n",
    "        else:\n",
    "            if self.heap_list[i * 2] < self.heap_list[i * 2 + 1]:\n",
    "                return i * 2\n",
    "            else:\n",
    "                return i * 2 + 1\n",
    "            \n",
    "    def map_swap(self, i, j):\n",
    "        #t = len(self.map)\n",
    "        #ind_i = \n",
    "        #ind_j = \n",
    "        self.map[ (self.heap_list[i][2], self.heap_list[i][3]) ],\\\n",
    "                    self.map[ (self.heap_list[j][2], self.heap_list[j][3]) ] = j, i\n",
    "#         if len(self.map) != t:\n",
    "#             print('ind_i, ind_j:',ind_i, ind_j)\n",
    "#             print(t, len(self.map))\n",
    "#             print('i, j:', i, j)\n",
    "#             print(self.heap_list)\n",
    "#             print(self.map)\n",
    "#             raise\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.current_size\n",
    "        \n",
    " \n",
    "    def delete_min(self):\n",
    "#         if len(self.heap_list) == 1:\n",
    "#             raise ValueError('empty heap')\n",
    " \n",
    "        root = self.heap_list[1]\n",
    "        self.heap_list[1], self.heap_list[self.current_size] = self.heap_list[self.current_size], self.heap_list[1]\n",
    "        \n",
    "        self.map_swap(1, self.current_size)\n",
    "        \n",
    "        del self.map[ (self.heap_list[self.current_size][2], self.heap_list[self.current_size][3]) ]\n",
    "        #self.map[ (self.heap_list[self.current_size][2], self.heap_list[self.current_size][3]) ] = None\n",
    "        \n",
    "        #del self.heap_list[self.current_size]\n",
    "        \n",
    "        #self.heap_list = self.heap_list[:-1]\n",
    "        #*self.heap_list, _ = self.heap_list\n",
    "        self.current_size -= 1\n",
    "        \n",
    "        #print('list:', self.heap_list)\n",
    "        #print('map:', self.map)\n",
    "        \n",
    "        self.sift_down(1)\n",
    "\n",
    "        return root\n",
    "    \n",
    "    def free(self):\n",
    "        del self.heap_list\n",
    "        del self.map\n",
    "        \n",
    "        self.heap_list = self.init # cur_weight, node_num, origin_i, preffix\n",
    "        self.current_size = 0\n",
    "        self.map = {(t[2], t[3]) : 0} # (origin_i, trie_node_num): heap_node_num\n",
    "        \n",
    "    def get_head_origin_index(self):\n",
    "        if self.curent_size > 0:\n",
    "            return self.heap_list[1][2]\n",
    "        return None\n",
    "\n",
    "# \"\"\"\n",
    "# Driver program\n",
    "# \"\"\"\n",
    "# # Same tree as above example.\n",
    "# my_heap = MinHeap()\n",
    "# my_heap.insert([-1, 0,4,'cd'])\n",
    "# my_heap.insert([-2, 1, 1, 'aa'])\n",
    "# my_heap.insert([-2, 2, 2, 'bb'])\n",
    "# my_heap.insert([-0.1, 0, 0, 'cc'])\n",
    "# my_heap.insert([-0.3, 0,0,'cc'])\n",
    "# my_heap.insert([-4.5, 0,0,'cc'])\n",
    "# my_heap.insert([-0.1, 0,0,'cc'])\n",
    "\n",
    "# print(my_heap.heap_list)\n",
    "# print(my_heap.delete_min()) # removing min node i.e 5 \n",
    "# print(my_heap.delete_min())\n",
    "# print(my_heap.delete_min())\n",
    "# print(my_heap.delete_min())\n",
    "# print(my_heap.delete_min())\n",
    "# print(my_heap.delete_min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нечеткий поиск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_search(trie, origin, letter_dict, max_heap_size=100, max_ans_size=10, alpha=1): # absf, absd\n",
    "    lev = Levenstein(letter_dict)\n",
    "    heap = MinHeap(max_heap_size) # [(0, 0, 0, '')] # cur_weight, node_num, origin_i, preffix\n",
    "    heap.insert((0, 0, 0, ''))\n",
    "    \n",
    "    #ans = MinHeap(max_ans_size) # origin_i в конце, вершина в боре - терминальная. Соответствующий префикс - ответ\n",
    "    #ans = set()\n",
    "    ans = dict()\n",
    "    #fixed = ''.join(('^', fixed))\n",
    "    \n",
    "    while len(ans) < max_heap_size and len(heap) > 0:\n",
    "        \n",
    "        #new_heap = MinHeap(max_heap_size) # []\n",
    "        cur_weight, node_num, origin_i, fixed_pref = heap.delete_min() # heapq.heappop(heap)\n",
    "        #print(len(heap), len(ans), node_num, origin_i)\n",
    "        \n",
    "        if origin_i == len(origin):\n",
    "            if trie.data[node_num].is_term:\n",
    "                #ans.insert( (cur_weight, node_num, origin_i, fixed_pref) )\n",
    "                #ans.add( (cur_weight, node_num, origin_i, fixed_pref) )\n",
    "                ans[(node_num, origin_i, fixed_pref)] = min( ans.get((node_num, origin_i, fixed_pref), 1e9), cur_weight )\n",
    "                \n",
    "                if len(ans) >= max_ans_size:\n",
    "                    break\n",
    "            continue\n",
    "        \n",
    "        children = trie.data[node_num].children\n",
    "        \n",
    "        if len(children) > 0:\n",
    "            prior = lev.get_distance(origin[:origin_i + 1], fixed_pref)\n",
    "            weight = math.log( prior ) # alpha * math.log( trie.data[child_node_num].freq + 1 )\n",
    "            # TODO: возвращать ответ без экспоненты, а то лишние действия \n",
    "            #print(prior, weight)\n",
    "            #return\n",
    "            heap.insert( (-weight + cur_weight, node_num, origin_i + 1, fixed_pref))\n",
    "\n",
    "        for symb in children:\n",
    "            child_node_num = children[symb]\n",
    "\n",
    "            prior = lev.get_distance(origin[:origin_i + 1], ''.join((fixed_pref, symb)) )\n",
    "            #print(alpha * math.log( trie.data[child_node_num].freq + 1 ), math.log( prior ))\n",
    "            weight = alpha * math.log( trie.data[child_node_num].freq + 1 ) + math.log( prior )\n",
    "            heap.insert( (-weight + cur_weight, child_node_num, origin_i + 1, ''.join((fixed_pref, symb)) ) )\n",
    "            #new_heap.insert( (-weight + cur_weight, child_node_num, origin_i + 1, fixed_pref + symb) )\n",
    "            \n",
    "            prior = lev.get_distance(origin[:origin_i], ''.join((fixed_pref, symb)) )\n",
    "            weight = alpha * math.log( trie.data[child_node_num].freq + 1 ) + math.log( prior )\n",
    "            heap.insert( (-weight + cur_weight, child_node_num, origin_i, ''.join((fixed_pref, symb))) )\n",
    "            \n",
    "        #print(heap.heap_list)\n",
    "        #print()\n",
    "            #heapq.heappush(new_heap, (-weight + cur_weight, child_node_num, origin_i + 1, fixed_pref + symb))\n",
    "\n",
    "#             prior = lev.get_distance(origin[:origin_i], fixed_pref + symb)\n",
    "#             weight = alpha * math.log( trie.data[child_node_num].freq + 1 ) - math.log( prior ) # TODO: + вероятность\n",
    "#             heapq.heappush(new_heap, (-weight + cur_weight, child_node_num, origin_i, fixed_pref + symb))\n",
    "            \n",
    "            # TODO: а нужно ли сделать alpha * m ? Вроде учли уже этот переход alpha * math.log( trie.data[node_num].freq + 1 ) \n",
    "#             prior = lev.get_distance(origin[:origin_i + 1], fixed_pref)\n",
    "#             weight = -math.log( prior ) # TODO: + вероятность\n",
    "#             heapq.heappush(new_heap, (-weight + cur_weight, node_num, origin_i + 1, fixed_pref))\n",
    "            \n",
    "            #new_heap = new_heap[:max_heap_size]\n",
    "\n",
    "        #print('new_heap:', new_heap.heap_list)\n",
    "        #heap.merge(new_heap)\n",
    "        #heap += new_heap\n",
    "        #heapq.heapify(heap)\n",
    "        #heap.cut()\n",
    "    \n",
    "    return sorted([(ans[key],) + key[2:3] for key in ans])\n",
    "    #return ans.heap_list[1 : ans.current_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3020783696943161\n",
      "0.09362328715381071\n"
     ]
    }
   ],
   "source": [
    "print(lev.get_distance('аидро', 'андро'))\n",
    "print(lev.get_distance('аидро', 'видео'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 973 ms, sys: 57.3 ms, total: 1.03 s\n",
      "Wall time: 1.12 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(-13.800681390039863, 'проспект'),\n",
       " (5.279304193144733, 'просвет'),\n",
       " (9.616674755352989, 'просвети')]"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# TODO: каждый del заменить на проверку того, лучше получившийся скор или хуже \n",
    "#sugg = list(fuzzy_search(trie, 'аидро', letter_dict, alpha=0.5, max_heap_size=50, max_ans_size=3))\n",
    "#sugg = fuzzy_search(trie, 'смтрть', letter_dict, alpha=0.5, max_heap_size=50, max_ans_size=3) #.heap_list[1:] # аидро\n",
    "sugg = fuzzy_search(trie, 'праспект', letter_dict, alpha=0.45, max_heap_size=50, max_ans_size=3)\n",
    "#sugg = fuzzy_search(trie, 'проспект', letter_dict, alpha=0.45, max_heap_size=100, max_ans_size=3)\n",
    "sugg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"qй\n",
    "wц\n",
    "eу\n",
    "rк\n",
    "tе\n",
    "yн\n",
    "uг\n",
    "iш\n",
    "oщ\n",
    "pз\n",
    "[х\n",
    "]ъ\n",
    "aф\n",
    "sы\n",
    "dв\n",
    "fа\n",
    "gп\n",
    "hр\n",
    "jо\n",
    "kл\n",
    "lд\n",
    ";ж\n",
    "\\\\э\n",
    "'ё\n",
    "zя\n",
    "xч\n",
    "cс\n",
    "vм\n",
    "bи\n",
    "nт\n",
    "mь\n",
    ",б\n",
    ".ю\n",
    "//\n",
    "`]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ru = dict()\n",
    "ru_en = dict()\n",
    "for p in s.split():\n",
    "    p = p.strip()\n",
    "    en_ru[p[0]] = p[1]\n",
    "    ru_en[p[1]] = p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_ru.pkl', 'wb') as output:\n",
    "    pickle.dump(en_ru, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open('ru_en.pkl', 'wb') as output:\n",
    "    pickle.dump(ru_en, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_checker_layout(query, words_prior_dict):\n",
    "    query = query.split()\n",
    "    new_query = \"\"\n",
    "    cand_ru, cand_en = [], []\n",
    "    \n",
    "    for word in query:\n",
    "        word_ru, word_en = \"\", \"\"\n",
    "        for symb in word:\n",
    "            word_ru += en_ru.get(symb, symb)\n",
    "            word_en += ru_en.get(symb, symb)\n",
    "        cand_ru.append(word_ru)\n",
    "        cand_en.append(word_en)\n",
    "    \n",
    "    ru_cnt, en_cnt = 0, 0\n",
    "    for word_ru, word_en in zip(cand_ru, cand_en):\n",
    "        p_ru = words_prior_dict.get(word_ru, 0)\n",
    "        p_en = words_prior_dict.get(word_en, 0)\n",
    "        if p_ru > p_en:\n",
    "            new_query = ' '.join( (new_query, word_ru) )\n",
    "            ru_cnt += 1\n",
    "        elif p_en > p_ru:\n",
    "            new_query = ' '.join( (new_query, word_en) )\n",
    "            en_cnt += 1\n",
    "        else:\n",
    "            new_query = ' '.join( (new_query, word_en if en_cnt > ru_cnt else word_ru) )\n",
    "            \n",
    "    \n",
    "    return new_query[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'смтреть кино онлайн бесплатно'"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_checker_layout('cvnhtnm rbно онлайн беспkfnyj', words_prior_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'change layout метод'"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_checker_layout('срфтпу дфнщге метод', words_prior_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Бор:\n",
    "- клеим слово 1 и слово 2, ищем его в боре\n",
    "\n",
    "- готовые слова\n",
    "- неготовый префикс\n",
    "- индекс текущего нерассмотренного слова\n",
    "-> min heap\n",
    "\n",
    "\n",
    "- берем текущий вариант - префикс, индекс первого нерассмотренного слова\n",
    "     -терм? добавим то что есть уже и индекс след слова\n",
    "     - не терм? склеиваем со след или видим, что вариантов нет.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        \n",
    "def split_search(trie, words, letter_dict, max_heap_size=100, max_ans_size=3, alpha=1):\n",
    "    words = prepare_query(words) # TODO\n",
    "    \n",
    "    heap = QueryMinHeap(max_heap_size, init=[ (1e8, (), '', 0, 0) ]) #\n",
    "    heap.insert((-trie.data[0].freq, (), '', 0, 0)) # \n",
    "    end_ind = len(words)\n",
    "    \n",
    "    ans = dict()\n",
    "    \n",
    "    while len(ans) < max_heap_size and len(heap) > 0:    \n",
    "        \n",
    "        # 0, [], '', 0, 0\n",
    "        score, ready_list, preffix, cur_word_ind, cur_node_num = heap.delete_min()\n",
    "        \n",
    "        if cur_word_ind == end_ind:\n",
    "            if trie.data[cur_node_num].is_term:\n",
    "                ans[ready_list] = min( ans.get(ready_list, 1e9), score )\n",
    "                \n",
    "                if len(ans) >= max_ans_size:\n",
    "                    break\n",
    "            continue\n",
    "        \n",
    "        add = trie.data[cur_node_num].freq\n",
    "        for symb in words[cur_word_ind]: # продолжаем префикс \n",
    "            children = trie.data[cur_node_num].children\n",
    "            cur_node_num = children.get(symb, -1)\n",
    "            if cur_node_num == -1:\n",
    "                break\n",
    "        if cur_node_num == -1:\n",
    "            continue\n",
    "            \n",
    "#         print(heap.heap_list)\n",
    "#         print()\n",
    "                \n",
    "        if trie.data[cur_node_num].is_term:\n",
    "            new_score = score - trie.data[cur_node_num].freq\n",
    "            heap.insert( (new_score, ready_list + (''.join( (preffix, words[cur_word_ind]) ), ), '',\n",
    "                          cur_word_ind + 1, 0, ) )\n",
    "            \n",
    "        if cur_word_ind + 1 < end_ind:\n",
    "            new_score = score + add - trie.data[cur_node_num].freq\n",
    "            heap.insert( (new_score, ready_list, ''.join( (preffix, words[cur_word_ind]) ),\n",
    "                          cur_word_ind + 1, cur_node_num, ) )\n",
    "    \n",
    "    #return ans\n",
    "    return sorted([ (-ans[key],)  + (key,) for key in ans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[(-751869, ('прохождение', 'игры', 'онлайн')), (-748413, ('про', 'хождение', 'игры', 'онлайн')), (-375547, ('прохождение', 'игры', 'он', 'лайн')), (-375247, ('прохождение', 'иг', 'ры', 'онлайн')), (-372091, ('про', 'хождение', 'игры', 'он', 'лайн')), (-371791, ('про', 'хождение', 'иг', 'ры', 'онлайн')), (1075, ('прохождение', 'иг', 'ры', 'он', 'лайн')), (4531, ('про', 'хождение', 'иг', 'ры', 'он', 'лайн'))]\n"
     ]
    }
   ],
   "source": [
    "res = split_search(trie, 'про хож дение иг ры он лайн', letter_dict, max_ans_size=10)\n",
    "print(len(res))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates_split(query, max_size=10, max_ans_size=10, alpha=1.0, gamma=1.0, return_query=True):\n",
    "    word_candidates = split_search(trie, query, letter_dict, alpha=alpha, max_heap_size=100, max_ans_size=max_ans_size)\n",
    "    scores = []\n",
    "\n",
    "    for i, pair in enumerate(word_candidates):\n",
    "        word1 = pair[1][0]\n",
    "        cur_score = words_prior_dict.get(word1, 0)\n",
    "        \n",
    "        eps = 1e-8\n",
    "        for j, word2 in enumerate(pair[1][1:]):\n",
    "            #print(word1, word2)\n",
    "            path_weight = -gamma * math.log( 1 + bigramm_words_dict.get('_'.join((word1, word2)), 0) )\n",
    "            cur_score = cur_score + path_weight - alpha * math.log(words_prior_dict.get(word2, eps)) #alpha * math.log(1 + words_prior_dict.get(word2, 0))\n",
    "            word1 = word2\n",
    "        scores.append( (cur_score, pair[1]) )\n",
    "    \n",
    "\n",
    "    return sorted(scores)[:max_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0005527122760585237, ('проспект',))]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_candidates_split('про хож дение иг ры он лайн')\n",
    "#get_candidates_split('log out icloud without password forum')\n",
    "#get_candidates_split('про спект')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8519 ['5 go set list', '5 go setlist\\n']\n",
      "12670 ['64 x 64 icons', '64 x64 icons\\n']\n",
      "18596 ['S 84 site:.io', 'S84 site:.io\\n']\n",
      "28230 ['postcode usa', 'post code usa\\n']\n",
      "45269 ['save from net', 'savefrom net\\n']\n",
      "55165 ['log out icloud without password forum', 'logout icloud without password forum\\n']\n",
      "55751 ['ad block', 'adblock\\n']\n",
      "62266 ['save from net', 'savefrom net\\n']\n",
      "63214 ['lm 1036', 'lm1036\\n']\n",
      "64930 ['C 91 site:.at', 'C91 site:.at\\n']\n",
      "65292 ['D 91 site:.be', 'D91 site:.be\\n']\n",
      "71317 ['KB 2919355', 'KB2919355\\n']\n",
      "72159 ['R 91 site:.li', 'R91 site:.li\\n']\n",
      "72869 ['ПрохождениеThe Visitor Returns', 'Прохождение The Visitor Returns\\n']\n",
      "74345 ['gamedata.db 7', 'gamedata.db7\\n']\n",
      "93406 ['E 93 site:.sa', 'E93 site:.sa\\n']\n",
      "98508 ['c cliner', 'ccleaner\\n']\n",
      "107241 ['text-uri news reporters bugged', 'texture news reporters bugged\\n']\n",
      "117676 ['anti bot protection', 'antibot protection\\n']\n",
      "130578 ['max core', 'maxcore\\n']\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate(hard_queries[:20]):\n",
    "    print(line[0], line[1].split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryMinHeap:\n",
    "    def __init__(self, max_heap_size, init=[-1e8, [-1]], key_ind=1):\n",
    "        \"\"\"\n",
    "        On this implementation the heap list is initialized with a value\n",
    "        \"\"\"\n",
    "        self.init = init\n",
    "        self.heap_list = [init] + [None] * max_heap_size # cur_weight, node_num, origin_i, preffix\n",
    "        self.current_size = 0\n",
    "        self.map = dict() #{(init[2], init[3]) : 0} # (origin_i, preffix): heap_node_num\n",
    "        self.max_heap_size = max_heap_size\n",
    "        self.key_ind = key_ind\n",
    "\n",
    "    def merge(self, heap_b):\n",
    "        for elem in heap_b.heap_list[1 : heap_b.current_size + 1]:\n",
    "            self.insert(elem)\n",
    " \n",
    "    def sift_up(self, i):\n",
    "        \"\"\"\n",
    "        Moves the value up in the tree to maintain the heap property.\n",
    "        \"\"\"\n",
    "        #print('sift_up')\n",
    "        while i // 2 > 0:\n",
    "            if self.heap_list[i][0] < self.heap_list[i // 2][0]:\n",
    "                self.map_swap(i, i // 2)\n",
    "                self.heap_list[i], self.heap_list[i // 2] = self.heap_list[i // 2], self.heap_list[i]\n",
    "            i = i // 2\n",
    " \n",
    "\n",
    "    def insert(self, k):\n",
    "        \"\"\"\n",
    "        Inserts a value into the heap\n",
    "        \"\"\"\n",
    "        #print(my_heap.heap_list, self.map.get((k[2], k[1]), None))\n",
    "        \n",
    "        heap_node_num = self.map.get(k[self.key_ind], None)\n",
    "        if heap_node_num is None:\n",
    "            self.current_size += 1\n",
    "            self.heap_list[self.current_size] = k\n",
    "            self.map[k[self.key_ind]] = self.current_size\n",
    "            \n",
    "            self.sift_up(self.current_size)\n",
    "            if self.current_size >= self.max_heap_size:\n",
    "                del self.map[ self.heap_list[self.current_size][self.key_ind] ]\n",
    "                self.current_size -= 1\n",
    "\n",
    "        elif k[0] < self.heap_list[heap_node_num][0]:\n",
    "            self.heap_list[heap_node_num] = k\n",
    "            self.sift_up(heap_node_num)\n",
    "\n",
    " \n",
    "    def sift_down(self, i):\n",
    "        #print('sift_down')\n",
    "        while i * 2 <= self.current_size:\n",
    "            mc = self.min_child(i)\n",
    "            if self.heap_list[i][0] > self.heap_list[mc][0]:\n",
    "                self.map_swap(i, mc)\n",
    "                self.heap_list[i], self.heap_list[mc] = self.heap_list[mc], self.heap_list[i]\n",
    "                i = mc\n",
    "            else:\n",
    "                break\n",
    " \n",
    "    def min_child(self, i):\n",
    "        if i * 2 + 1 > self.current_size:\n",
    "            return i * 2\n",
    "        else:\n",
    "            if self.heap_list[i * 2] < self.heap_list[i * 2 + 1]:\n",
    "                return i * 2\n",
    "            else:\n",
    "                return i * 2 + 1\n",
    "            \n",
    "    def map_swap(self, i, j):\n",
    "        #t = len(self.map)\n",
    "        #ind_i = \n",
    "        #ind_j = \n",
    "        self.map[ self.heap_list[i][self.key_ind] ], self.map[ self.heap_list[j][self.key_ind] ] = j, i\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.current_size\n",
    "        \n",
    " \n",
    "    def delete_min(self):\n",
    "#         if len(self.heap_list) == 1:\n",
    "#             raise ValueError('empty heap')\n",
    " \n",
    "        root = self.heap_list[1]\n",
    "        self.heap_list[1], self.heap_list[self.current_size] = self.heap_list[self.current_size], self.heap_list[1]\n",
    "        \n",
    "        self.map_swap(1, self.current_size)\n",
    "        \n",
    "        del self.map[ self.heap_list[self.current_size][self.key_ind] ]\n",
    "\n",
    "        self.current_size -= 1\n",
    "        \n",
    "        self.sift_down(1)\n",
    "\n",
    "        return root\n",
    "#     def free(self):\n",
    "#         del self.heap_list\n",
    "#         del self.map\n",
    "        \n",
    "#         self.heap_list = self.init # cur_weight, node_num, origin_i, preffix\n",
    "#         self.current_size = 0\n",
    "#         self.map = {(t[2], t[3]) : 0} # (origin_i, trie_node_num): heap_node_num\n",
    "        \n",
    "#     def get_head_origin_index(self):\n",
    "#         if self.curent_size > 0:\n",
    "#             return self.heap_list[1][2]\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hp = QueryMinHeap(10)\n",
    "# t = [1]\n",
    "# hp.insert(((-1, '1asd'), tuple(t)))\n",
    "# t = [1,1]\n",
    "# hp.insert(((-0.5, 'asdf'), tuple(t)))\n",
    "# t = [1,1,3]\n",
    "# hp.insert(((1, '2qw'), tuple(t)))\n",
    "# hp.delete_min()\n",
    "# hp.delete_min()\n",
    "# hp.delete_min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кандидаты на исправление "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b99e2c4eed94a0d8579e6d64acab073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1670701.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "second_words = dict()\n",
    "for key in tqdm(bigramm_words_dict):\n",
    "    word1, word2 = key.split('_')\n",
    "    second_words[word2] = second_words.get(word2, 0) + bigramm_words_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramm_words_dict2 = copy.copy(bigramm_words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramm_words_dict = copy.copy(bigramm_words_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b96d6566ff4ec0846e1c1779a54aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1670701.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for key in tqdm(bigramm_words_dict):\n",
    "    word1, word2 = key.split('_')\n",
    "    bigramm_words_dict[key] = bigramm_words_dict[key] / second_words[word2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(query, search_func=fuzzy_search,\n",
    "                   max_size=10, max_ans_size=3, alpha=0.1, gamma=1.0, return_query=True):\n",
    "    words = prepare_query(query)\n",
    "    word_candidates = []\n",
    "    for word in words:\n",
    "        # TODO: проигнорировать числа\n",
    "        sugg = search_func(trie, word, letter_dict, alpha=alpha, max_heap_size=50, max_ans_size=max_ans_size)\n",
    "        word_candidates.append( sugg ) # min weight => best\n",
    "    \n",
    "    heap = QueryMinHeap(max_size)\n",
    "    for i, cand in enumerate(word_candidates[0]):\n",
    "        heap.insert( (cand[0], (i, )) )\n",
    "\n",
    "    ans = dict()\n",
    "    #heapq.heapify(heap)\n",
    "    while len(ans) < max_size and len(heap) > 0:\n",
    "        weight, cur_seq = heap.delete_min()\n",
    "        if len(cur_seq) >= len(words):\n",
    "            ans[cur_seq] = min(ans.get(cur_seq, 1e9), weight)\n",
    "            continue\n",
    "        \n",
    "        for j, (cand_weight, word2) in enumerate(word_candidates[len(cur_seq)]):\n",
    "            word1 = word_candidates[len(cur_seq) - 1][cur_seq[-1]][1]\n",
    "            #print(word1, word2)\n",
    "            #path_weight = -gamma * math.log( 1 + bigramm_words_dict.get('_'.join((word1, word2)), 0) )\n",
    "            eps = 1e-8\n",
    "            path_weight = -gamma * math.log( bigramm_words_dict.get('_'.join((word1, word2)), eps) )\n",
    "            #print( weight, path_weight, cand_weight ) \n",
    "            res = weight + path_weight + cand_weight\n",
    "            \n",
    "            heap.insert((res, cur_seq + (j, ) ))\n",
    "    \n",
    "    if return_query:\n",
    "        res = []\n",
    "        for key in ans:\n",
    "            q = ' '.join(word_candidates[j][elem][1] for j, elem in enumerate(key))\n",
    "            res.append((ans[key], q))\n",
    "        return sorted(res)\n",
    "    \n",
    "    return sorted([(ans[key], key) for key in ans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.21 s, sys: 544 ms, total: 1.75 s\n",
      "Wall time: 2.72 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(-7.964893232381435, 'просвет мира'), (-5.540999231924614, 'просвети мира')]"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#get_candidates('смтрть онлан', max_size=10, max_ans_size=5, alpha=0.4, gamma=1.0)\n",
    "get_candidates('проспект мира', max_size=3, max_ans_size=3, alpha=0.5, gamma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_stat(fname='queries_all.txt'):\n",
    "    query_dict = dict()\n",
    "    \n",
    "    with open(fname) as fin:    \n",
    "        \n",
    "        for i, line in tqdm(enumerate(fin)):\n",
    "            \n",
    "            pair = line.lower().split('\\t')\n",
    "            \n",
    "            tokens = prepare_query( pair[0] )\n",
    "            #alpha_tokens = list(filter(lambda s: s.isalpha(), tokens))\n",
    "            if len(tokens) == 0:\n",
    "                continue\n",
    "            query = ' '.join( tokens )\n",
    "\n",
    "            prev = query_dict.get(query, (0, True))\n",
    "            query_dict[query] = ( prev[0] + 1, prev[1] and (len(pair) > 1) )\n",
    "    return query_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959c4497792846cebfcf5ad875e616ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query_dict = get_query_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('query_dict.pkl', 'wb') as output:\n",
    "#     pickle.dump(query_dict, output, pickle.HIGHEST_PROTOCOL)\n",
    "import pickle\n",
    "with open('query_dict.pkl', 'rb') as fin:\n",
    "    query_dict = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "TETA = 0.01\n",
    "\n",
    "def get_features_train(query_dict, fname='queries_all.txt', stop_iter=500, teta=TETA):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    with open(fname) as fin:\n",
    "        fix_iter, origin_iter = stop_iter, stop_iter\n",
    "        for i, line in tqdm(enumerate(fin)):\n",
    "\n",
    "            pair = line.split('\\t')\n",
    "\n",
    "            tokens = prepare_query(pair[0])\n",
    "            alpha_tokens = list(filter(lambda s: s.isalpha(), tokens))\n",
    "            if len(alpha_tokens) == 0:\n",
    "                continue\n",
    "            query = ' '.join(alpha_tokens)\n",
    "\n",
    "            if origin_iter == 0 and fix_iter == 0:\n",
    "                break\n",
    "\n",
    "            if len(pair) > 1:\n",
    "                if fix_iter == 0:\n",
    "                    continue\n",
    "                fix_iter -= 1\n",
    "            else:\n",
    "                if origin_iter == 0:\n",
    "                    continue\n",
    "                origin_iter -= 1\n",
    "\n",
    "            # eng_symb_cnt = 0\n",
    "            # for symb in query:\n",
    "            #     eng_symb_cnt += ('a' <= symb <= 'z')\n",
    "            #\n",
    "            # symb_cnt = 0\n",
    "            # for symb in query:\n",
    "            #     symb_cnt += symb.isalpha()\n",
    "            #\n",
    "            # eng_part = eng_symb_cnt / symb_cnt\n",
    "\n",
    "            corr_score, corr_query = get_candidates(query)[0]\n",
    "            corr_tokens = corr_query.split()\n",
    "\n",
    "            null_words = 0\n",
    "            for word in corr_tokens:\n",
    "                if word not in words_prior_dict:\n",
    "                    null_words += 1\n",
    "\n",
    "            #bigramm_prior = 0\n",
    "            null_bigramm = 0\n",
    "            for word1, word2 in zip(corr_tokens, corr_tokens[1:]):\n",
    "                p = bigramm_words_dict.get('_'.join((word1, word2)), 0)\n",
    "                null_bigramm += (p == 0)\n",
    "            #     bigramm_prior += math.log(1 + p)\n",
    "            #\n",
    "            # bigramm_prior += teta * math.log(1 + LEN_WORDS_DICT * words_prior_dict.get(corr_tokens[-1], 0))\n",
    "\n",
    "            X.append( [null_bigramm, null_words] )\n",
    "            # X.append(\n",
    "            #     [len(alpha_tokens), len(tokens), len(query), symb_cnt, eng_part, null_bigramm, null_words,\n",
    "            #      corr_score, bigramm_prior, query_dict.get(query, (0, False))[0] / len(query_dict)]\n",
    "            # )\n",
    "            y.append(1 if len(pair) > 1 else 0)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "\n",
    "def get_features_test(query, cand, tokens, query_dict, words_prior_dict, stop_iter=500, teta=TETA):\n",
    "    \n",
    "    # alpha_tokens = list(filter(lambda s: s.isalpha(), tokens))\n",
    "    # prep_query = ' '.join(alpha_tokens)\n",
    "\n",
    "    # eng_symb_cnt = 0\n",
    "    # for symb in query:\n",
    "    #     eng_symb_cnt += ('a' <= symb <= 'z')\n",
    "    #\n",
    "    # symb_cnt = 0\n",
    "    # for symb in query:\n",
    "    #     symb_cnt += symb.isalpha()\n",
    "    #\n",
    "    # eng_part = eng_symb_cnt / symb_cnt\n",
    "\n",
    "    corr_score, corr_query = cand\n",
    "    corr_tokens = corr_query.split()\n",
    "\n",
    "    null_words = 0\n",
    "    for word in corr_tokens:\n",
    "        if word not in words_prior_dict:\n",
    "            null_words += 1\n",
    "\n",
    "    #bigramm_prior = 0\n",
    "    null_bigramm = 0\n",
    "\n",
    "    for word1, word2 in zip(corr_tokens, corr_tokens[1:]):\n",
    "        p = bigramm_words_dict.get('_'.join((word1, word2)), 0)\n",
    "        null_bigramm += (p == 0)\n",
    "    #     bigramm_prior += math.log(1 + p)\n",
    "    #\n",
    "    # bigramm_prior += teta * math.log(1 + LEN_WORDS_DICT * words_prior_dict.get(corr_tokens[-1], 0))\n",
    "\n",
    "    X = [[null_bigramm, null_words]]\n",
    "    # X = [[len(alpha_tokens), len(tokens), len(query), symb_cnt, eng_part, null_bigramm, null_words,\n",
    "    #       corr_score, bigramm_prior, query_dict.get(prep_query, (0, False))[0] / len(query_dict)]]\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_features_train(stop_iter=200, query_dict=query_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. равны ли запрос и кандидат\n",
    "2. исправляли ли до этого такой запрос\n",
    "3. модель\n",
    "\"\"\"\n",
    "Xt = get_features_test('мама мыла раму', (0, 'мама мыла раму'), tokens='мама мыла раму'.split(),\n",
    "                       query_dict=query_dict, words_prior_dict=words_prior_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фичи классификатора:\n",
    "- количество слов\n",
    "- длина в символах\n",
    "- частотность запроса\n",
    "- вес исправления\n",
    "- есть ли английские буквы\n",
    "- вес исправления \n",
    "- биграммная вероятность слов в исправленном запросе vs неисправленном\n",
    "\n",
    "\n",
    "- SVM / logreg / нейронка полносвязная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler as SS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "from sklearn.ensemble import RandomForestClassifier as RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(np.array(X)[:, 5:7], y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = SS()\n",
    "scaler.fit(np.array(X)[:, 5:7])\n",
    "X_train0 = scaler.transform(X_train)\n",
    "model = LogReg()\n",
    "model.fit(X_train0, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6391752577319588\n",
      "1.0\n",
      "0.4696969696969697\n"
     ]
    }
   ],
   "source": [
    "#model = LogReg() # SVC() # \n",
    "#model.fit(X, y)\n",
    "y_pred = model.predict(X_val)\n",
    "print( f1_score(y_val, y_pred) )\n",
    "print( recall_score(y_val, y_pred) )\n",
    "print( precision_score(y_val, y_pred, ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8387096774193549\n",
    "0.8387096774193549\n",
    "0.8387096774193549\n",
    "\n",
    "0.7213114754098361\n",
    "0.7333333333333333\n",
    "0.7096774193548387\n",
    "\n",
    "0.7419354838709677\n",
    "0.7419354838709677\n",
    "0.7419354838709677"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model1.pkl', 'wb') as output:\n",
    "    pickle.dump(model, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open('scaler1.pkl', 'wb') as output:\n",
    "    pickle.dump(scaler, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(stop_iter=500):\n",
    "    #X, y = get_features_train(stop_iter=stop_iter)\n",
    "    scaler = SS()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    model = LogReg()\n",
    "    model.fit(X, y)\n",
    "    return model, scaler\n",
    "\n",
    "# New!\n",
    "def predict(query, cand, model, scaler, query_dict):\n",
    "    \n",
    "    tokens = prepare_query( query )\n",
    "#     alpha_tokens = list(filter(lambda s: s.isalpha(), tokens))\n",
    "    if len(tokens) == 0:\n",
    "        return False\n",
    "    \n",
    "    query = ' '.join( tokens )\n",
    "    \n",
    "    if query == cand:\n",
    "        return False\n",
    "    \n",
    "    query_dict_val, was_fixed = query_dict.get(query, (0, False))\n",
    "    if was_fixed:\n",
    "        return True\n",
    "    \n",
    "    X_test = get_features_test(query, tokens=tokens, query_dict_val=query_dict_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    need_to_fix = model.predict(X_test)\n",
    "    \n",
    "    return need_to_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = 'беременною порно'\n",
    "predict(q, model, scaler, query_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, True)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_dict.get(q, (0, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal(query, cand):\n",
    "    return prepare_query(query) == prepare_query(cand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Одна компонента\n",
    "def spell_checker(query, model, scaler, query_dict, max_size=3):\n",
    "    candidates = get_candidates(query, max_size=max_size, max_ans_size=1, alpha=0.1, gamma=1.0)\n",
    "    if equal(query, candidates[0][1]):\n",
    "        return False, query # или candidates[0]\n",
    "    \n",
    "    for cand in candidates:\n",
    "        X_test = get_features_test(query, query_dict=query_dict)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        need_to_fix = model.predict(X_test)\n",
    "        if need_to_fix:\n",
    "            return need_to_fix, cand[1]\n",
    "        \n",
    "    return need_to_fix, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model=None, scaler=None, query_dict=None, n_iter=3):\n",
    "    if model is None:\n",
    "        model, scaler = get_model(stop_iter=500)\n",
    "    if query_dict is None:\n",
    "        query_dict = get_query_stat()\n",
    "\n",
    "    while True:\n",
    "        query = input()\n",
    "        for i in range(n_iter):\n",
    "            need_to_fix, fixed = spell_checker(query, model, scaler, query_dict, max_size=3)\n",
    "            if not need_to_fix:\n",
    "                break\n",
    "            query = fixed\n",
    "        \n",
    "        print(fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ама ыла аму\n",
      "[1]\n",
      "*\n",
      "*\n",
      "->\n",
      "ама пла аму\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-272-713b04e938ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-271-aeb5990182f6>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model, scaler, query_dict, n_iter)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mneed_to_fix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspell_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "run(model, scaler, query_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Построить словарь слов из запросов по пробелам и знакам: там где нет исправлений - 0й индекс, иначе 1й - просто кол-во. \n",
    "- посмотреть на низкочастнотные слова, мб отсеять.\n",
    "- Отсеять спец символы и мб слова, где они посреди слова\n",
    "- посмотреть и мб отсеять слова и 1-2 букв\n",
    "- биграммы - делим на количество биграмм, слова - делим на количество слов.\n",
    "- words_prior_dict\n",
    "- biwords_prior_dict\n",
    "\n",
    "\n",
    "Модель ошибок: \n",
    "- строим dict биграмм исправлений(только для 1 символа пока что) через таблицу Левенштейна - количество исправлений каждой пары букв. Отдельная функция, которая return словарь биграммных исправлений. Берем его, обновляем общий словарь таких исправлений. Параллельно вести словарь количеств любых исправлений для каждой буквы, в самом конце поделить на эти числа. Это для коэффициентов.\n",
    "- Расстояние левенштейна. Можно класс с весами переходов + get_prior(). Веса переходов - на основе прошлого пункта. \n",
    "- вероятность: alpha^(-lev)\n",
    "- letter_dict, get_part_letter_dict, get_prior_letter_dict\n",
    "- class Levenstein\n",
    "\n",
    "\n",
    "- Модель языка(биграммная) - бьем на слова, считаем вероятности по формуле биграммной модели. Вероятности считать со сглаживанием Лапласа. Тоже dict пар последовательных слов с вероятностями\n",
    "\n",
    "\n",
    "- построить бор по списку слов. Класс, в котором хранится dict номер вершины:вершина. В вершине - номер вершины, список пар номер_сына, вес_сына, частота данного префикса(при построении высчитывается).\n",
    "- def build_trie -> dict, class Node \n",
    "\n",
    "\n",
    "- Генерация вариантов замены: нечеткий поиск в боре, обрезание по очереди с приоритетами до N объектов. Frequency - при построении бора считаем статистику слов. P(W|C) из модели ошибок. Считаем для каждого слова вес кандидатов, вес - сумма весов переходов в боре. Учитывать втч пропуск и вставку символов. Выкидать кандидатов, у которых вес < eps по итогу прохода через бор. Выкидывать по порогу отношения к весу макс кандидата\n",
    "- ...\n",
    "- #fuzzy_search \n",
    "\n",
    "\n",
    "- генерация кандидатов замены для запроса - оптимальный путь в графе. Умножать вес ребра на вес вершины в которую переходим, в конце еще вероятность последнего слова мб(как в формуле).\n",
    "\n",
    "\n",
    "- классификатор fix/none на фичах\n",
    "\n",
    "\n",
    "- ранжирование: пока сортируем по весу ошибки.\n",
    "\n",
    "\n",
    "- сделать итерации \n",
    "\n",
    "\n",
    "Далее:\n",
    "- другие типы ошибок \n",
    "- Бор + soundex можно для кандидатов\n",
    "- мб из одного запроса брать только множество слов, тк там могут они повторятся и это вроде портит стату.\n",
    "- мб вероятность в левенштейне по-другому считать\n",
    "- мб только по исправленным запросам биграммы букв считать\n",
    "\n",
    "\n",
    "+ просмотреть еще раз конспект и прочекать разные дополнительные фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
