{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Качаем данные и создаём индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "record_len_bytes = 4\n",
    "opt_len = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAX_DOC_ID = int(1e9)\n",
    "docs_id = dict()\n",
    "doc_id_set = set([MAX_DOC_ID])\n",
    "doc_id_list = [MAX_DOC_ID]\n",
    "id_docs = dict()\n",
    "inv_index = dict()\n",
    "counter = 0\n",
    "index = dict()\n",
    "\n",
    "for i, filename in enumerate( os.listdir(data_path) ):\n",
    "    if filename.endswith('.gz'):\n",
    "    #if filename.startswith('lenta'):\n",
    "        path = '/'.join( (data_path, filename) )\n",
    "        with gzip.open(path, 'rb') as fin:\n",
    "        #with open(path, 'rb') as fin:\n",
    "            while True:\n",
    "                record_len = fin.read(record_len_bytes)\n",
    "                if len(record_len) == 0:\n",
    "                    break\n",
    "                \n",
    "                record_len = struct.unpack( 'I', record_len )[0]\n",
    "                buffer = fin.read(record_len)\n",
    "                \n",
    "                url_len = struct.unpack( 'B', buffer[1:2] )[0]\n",
    "                url = struct.unpack(''.join( (str(url_len), 's') ), buffer[2 : 2 + url_len])[0]\n",
    "                url = url.decode(\"utf-8\", \"ignore\")\n",
    "                opt = struct.unpack( 'H', buffer[2 + url_len : 2 + url_len + opt_len])[0]\n",
    "                \n",
    "                content_len = len(buffer) - (2 + url_len + opt_len)\n",
    "\n",
    "                if content_len == 0:\n",
    "                    continue\n",
    "                \n",
    "                content = struct.unpack( ''.join( (str(content_len - 1), 's') ), \n",
    "                                        buffer[2 + url_len + opt_len + 1:] )[0].decode(\"utf-8\", \"ignore\")\n",
    "                \n",
    "                content_tokens = list(set(map(lambda token: token.lower(), re.findall(r'\\w+', content))))\n",
    "                \n",
    "                if url in docs_id:\n",
    "                    print('duplicate', file=sys.stderr)\n",
    "                    continue\n",
    "                \n",
    "                id_docs[counter] = url\n",
    "                doc_id_set.add(counter)\n",
    "                doc_id_list.append(counter)\n",
    "                docs_id[url] = counter\n",
    "                index[counter] = content\n",
    "                \n",
    "                for token in content_tokens:\n",
    "                    if token not in inv_index:\n",
    "                        inv_index[token] = [MAX_DOC_ID]\n",
    "                    \n",
    "                    inv_index[token].append(counter)\n",
    "                          \n",
    "                counter += 1\n",
    "                \n",
    "\n",
    "index[MAX_DOC_ID] = \"---\"\n",
    "id_docs[MAX_DOC_ID] = \"---\"\n",
    "doc_id_list.sort()\n",
    "for key in inv_index:\n",
    "    # inv_index[key] = list(set(inv_index[key]))\n",
    "    inv_index[key].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarbyteEncoder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def encode(self, inv_index):\n",
    "        for key in tqdm(inv_index):\n",
    "            with open('/'.join(('index', key)), 'wb') as fout:\n",
    "                prev = inv_index[key][0]\n",
    "                res = self._encode_digit(prev)\n",
    "                for doc_id in inv_index[key][1:]:\n",
    "                    res += self._encode_digit(doc_id - prev)\n",
    "                    prev = doc_id\n",
    "                res += self._encode_digit(0) # end-of-list marker\n",
    "                fout.write(res)\n",
    "    \n",
    "    def _encode_digit(self, num):\n",
    "        res = b''\n",
    "        while num >= 128:\n",
    "            res += struct.pack('B', num % 128)\n",
    "            num //= 128\n",
    "        res += struct.pack('B', 128 + num)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = VarbyteEncoder()\n",
    "encoder.encode( inv_index )\n",
    "encoder.encode( {'_id_list': doc_id_list} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обратная польская запись"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rpn(tokens):\n",
    "    ops = {\n",
    "        '!' : 2,\n",
    "        '&' : 1,\n",
    "        '|' : 0,\n",
    "    }\n",
    "    res = []\n",
    "    stack = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token == '(':\n",
    "            stack.append('(')\n",
    "        elif token == ')':\n",
    "            while stack[-1] != '(':\n",
    "                res.append( stack.pop() )\n",
    "            stack.pop()\n",
    "        elif token == '!':\n",
    "            stack.append('!')\n",
    "        elif token in ops.keys():\n",
    "            while len(stack) > 0 and (stack[-1] == '!' or\n",
    "                                      stack[-1] in ops and ops[token] <= ops[stack[-1]]):\n",
    "                res.append( stack.pop() )\n",
    "            stack.append(token)\n",
    "        else:\n",
    "            res.append(token)\n",
    "\n",
    "    while len(stack) > 0:\n",
    "        res.append( stack.pop() )\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(query):\n",
    "    ops = ['!', '&', '|', '(', ')']\n",
    "    st = 0\n",
    "    en = 0\n",
    "    res = []\n",
    "    for c in query:\n",
    "        if c in ops:\n",
    "            tmp = query[st : en].strip()\n",
    "            if len(tmp) > 0:\n",
    "                res.append(tmp)\n",
    "            res.append(c)\n",
    "            st = en + 1\n",
    "        \n",
    "        en += 1\n",
    "    \n",
    "    tmp = query[st : en + 1].strip()\n",
    "    if len(tmp) > 0:\n",
    "        res.append(tmp)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, node_type, val=None, left_ind=-1, right_ind=-1):\n",
    "        \"\"\"\n",
    "        node_type:\n",
    "            1 - leaf - one query string\n",
    "            2 - not leaf - &|\n",
    "            3 - not leaf - ! - use only left child\n",
    "        \n",
    "        \"\"\"\n",
    "        self.type = node_type\n",
    "        self.left_ind = left_ind\n",
    "        self.right_ind = right_ind\n",
    "        self.parent = -1\n",
    "        self.val = val\n",
    "        \n",
    "    def whole_up(self, tree, inv_index, doc_id_set):\n",
    "        if self.type == 1:\n",
    "            self.doc_ids = set(inv_index.get(self.val, []))\n",
    "        elif self.val == '!':\n",
    "            self.doc_ids = doc_id_set - tree[self.left_ind].doc_ids\n",
    "        elif self.val == '&':\n",
    "            self.doc_ids = tree[self.left_ind].doc_ids & tree[self.right_ind].doc_ids\n",
    "        elif self.val == '|':\n",
    "            self.doc_ids = tree[self.left_ind].doc_ids | tree[self.right_ind].doc_ids\n",
    "        else:\n",
    "            print('Unknown bool operation', file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowNode:\n",
    "    def __init__(self, node_type, val=None, left_ind=-1, right_ind=-1):\n",
    "        \"\"\"\n",
    "        node_type:\n",
    "            1 - leaf - one query string\n",
    "            2 - not leaf - &|\n",
    "            3 - not leaf - ! - use only left child\n",
    "        \n",
    "        \"\"\"\n",
    "        self.type = node_type\n",
    "        self.left_ind = left_ind\n",
    "        self.right_ind = right_ind\n",
    "        self.parent = -1\n",
    "        self.val = val\n",
    "        \n",
    "        self.is_matched = False # TODO: сделать отдельно функцию иницилизации + проверки на консистентность\n",
    "        self.acceptable_doc_id = -1\n",
    "        if node_type == 1:\n",
    "            self.list_pos = -1\n",
    "    \n",
    "\n",
    "    def set_state(self, tree, inv_index, cur_doc_id, fantom_list):\n",
    "        if self.type == 1:\n",
    "            if self.val not in inv_index:\n",
    "                self.acceptable_doc_id = MAX_DOC_ID\n",
    "                self.is_matched = False\n",
    "            else:\n",
    "                while self.list_pos < len(inv_index[self.val]) and \\\n",
    "                            (self.list_pos < 0 or inv_index[self.val][self.list_pos] < cur_doc_id):\n",
    "                        self.list_pos += 1\n",
    "                fantom_list.update(cur_doc_id)\n",
    "                self.acceptable_doc_id = inv_index[self.val][self.list_pos]\n",
    "                self.is_matched = (self.acceptable_doc_id == cur_doc_id)\n",
    "            \n",
    "        elif self.val == '!':\n",
    "            if tree[self.left_ind].is_matched:\n",
    "                self.is_matched = False\n",
    "                self.acceptable_doc_id = fantom_list.get_next_id() #doc_id_list[fantom_list_pos + 1]\n",
    "            else:\n",
    "                if tree[self.left_ind].acceptable_doc_id == fantom_list.get_current_id():\n",
    "                                                        # doc_id_list[fantom_list_pos]:\n",
    "                    self.is_matched = False\n",
    "                    self.acceptable_doc_id = fantom_list.get_next_id() # doc_id_list[fantom_list_pos + 1] \n",
    "                else:\n",
    "                    self.is_matched = True\n",
    "                    self.acceptable_doc_id = fantom_list.get_current_id() # doc_id_list[fantom_list_pos]\n",
    "                    \n",
    "\n",
    "        elif self.val == '&':\n",
    "            self.acceptable_doc_id = max(tree[self.left_ind].acceptable_doc_id,\n",
    "                                        tree[self.right_ind].acceptable_doc_id)\n",
    "            self.is_matched = tree[self.left_ind].is_matched & tree[self.right_ind].is_matched\n",
    "                # (self.acceptable_doc_id == cur_doc_id)\n",
    "            \n",
    "        elif self.val == '|':\n",
    "            self.acceptable_doc_id = min(tree[self.left_ind].acceptable_doc_id,\n",
    "                                        tree[self.right_ind].acceptable_doc_id)\n",
    "            self.is_matched = tree[self.left_ind].is_matched | tree[self.right_ind].is_matched\n",
    "            #self.is_matched = (self.acceptable_doc_id == cur_doc_id)\n",
    "        else:\n",
    "            print('Unknown bool operation', file=sys.stderr)\n",
    "            \n",
    "            \n",
    "class FantomList:\n",
    "    def __init__(self, doc_id_list):\n",
    "        self.list_pos = -1\n",
    "        self.doc_id_list = doc_id_list\n",
    "    def update(self, cur_doc_id):\n",
    "        while self.list_pos < 0 or self.doc_id_list[self.list_pos] < cur_doc_id:\n",
    "            self.list_pos += 1\n",
    "    def get_current_id(self):\n",
    "        return self.doc_id_list[self.list_pos] if self.list_pos < len(self.doc_id_list) else MAX_DOC_ID\n",
    "    def get_next_id(self):\n",
    "        return self.doc_id_list[self.list_pos + 1] if self.list_pos + 1 < len(self.doc_id_list) else MAX_DOC_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_tree(query, NodeType):\n",
    "    \"\"\"\n",
    "    return: leafs, tree\n",
    "    \"\"\"\n",
    "    tree = dict()\n",
    "    last_node_key = 0\n",
    "    tokens = rpn( tokenize(query) )\n",
    "    \n",
    "    stack = []\n",
    "    leafs = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == '&' or token == '|':\n",
    "            tree[last_node_key] = NodeType(2, token)\n",
    "            node_id_right = stack.pop()\n",
    "            node_id_left = stack.pop()\n",
    "            tree[node_id_left].parent = last_node_key\n",
    "            tree[node_id_right].parent = last_node_key\n",
    "            tree[last_node_key].left_ind = node_id_left\n",
    "            tree[last_node_key].right_ind = node_id_right\n",
    "            stack.append(last_node_key)\n",
    "            \n",
    "        elif token == '!':\n",
    "            tree[last_node_key] = NodeType(3, token)\n",
    "            node_id_left = stack.pop()\n",
    "            tree[node_id_left].parent = last_node_key\n",
    "            tree[last_node_key].left_ind = node_id_left\n",
    "            stack.append(last_node_key)\n",
    "            \n",
    "        else:\n",
    "            tree[last_node_key] = NodeType(1, token)\n",
    "            stack.append(last_node_key)\n",
    "            leafs.append(last_node_key)\n",
    "        \n",
    "        last_node_key += 1\n",
    "    \n",
    "    if len(stack) != 1: # stack[-1] - root\n",
    "        return None, tree\n",
    "        \n",
    "    return stack[0], tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_bool_search(query, inv_index, doc_id_set):\n",
    "    root, tree = get_query_tree(query, Node)\n",
    "    if root is None or len(tree) == 0:\n",
    "        return []\n",
    "    stack = [root]\n",
    "    used = [False] * len(tree)\n",
    "    used[root] = True\n",
    "    \n",
    "    while len(stack) != 0:\n",
    "        if tree[stack[-1]].left_ind != -1 and not used[tree[stack[-1]].left_ind]:\n",
    "            stack.append(tree[stack[-1]].left_ind)\n",
    "            used[stack[-1]] = True\n",
    "        elif tree[stack[-1]].right_ind != -1 and not used[tree[stack[-1]].right_ind]:\n",
    "            stack.append(tree[stack[-1]].right_ind)\n",
    "            used[stack[-1]] = True\n",
    "        else:\n",
    "            tree[stack.pop()].whole_up(tree, inv_index, doc_id_set)\n",
    "        \n",
    "    return sorted(list(tree[root].doc_ids))[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_bool_search(query, inv_index, doc_id_list):\n",
    "    root, tree = get_query_tree(query, FlowNode)\n",
    "    if root is None or len(tree) == 0:\n",
    "        return []\n",
    "    posting_list = []\n",
    "    cur_doc_id = -1\n",
    "    \n",
    "    fantom_list = FantomList(doc_id_list)\n",
    "    \n",
    "    while cur_doc_id != MAX_DOC_ID:\n",
    "        \n",
    "        stack = [root]\n",
    "        used = [False] * len(tree)\n",
    "        used[root] = True\n",
    "        \n",
    "        while len(stack) != 0:\n",
    "            if tree[stack[-1]].left_ind != -1 and not used[tree[stack[-1]].left_ind]:\n",
    "                stack.append(tree[stack[-1]].left_ind)\n",
    "                used[stack[-1]] = True\n",
    "            elif tree[stack[-1]].right_ind != -1 and not used[tree[stack[-1]].right_ind]:\n",
    "                stack.append(tree[stack[-1]].right_ind)\n",
    "                used[stack[-1]] = True\n",
    "            else:\n",
    "                tree[stack.pop()].set_state(tree, inv_index, cur_doc_id, fantom_list)\n",
    "        \n",
    "        # print(tree[root].acceptable_doc_id, cur_doc_id)\n",
    "        \n",
    "        if tree[root].acceptable_doc_id == cur_doc_id:\n",
    "            posting_list.append(cur_doc_id)\n",
    "            cur_doc_id += 1\n",
    "        else:\n",
    "            cur_doc_id = tree[root].acceptable_doc_id\n",
    "\n",
    "    return posting_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(search_params=[flow_bool_search, doc_id_list]):\n",
    "    while True:\n",
    "        input_query = input()\n",
    "        query = input_query.strip().lower()\n",
    "        doc_ids = search_params[0](query, inv_index, search_params[1])\n",
    "        print(input_query)\n",
    "        print(len(doc_ids))\n",
    "        for doc_id in doc_ids:\n",
    "            print(id_docs[doc_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[621, 1103, 1779, 5362, 7759]"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_bool_search('мем | жопа', inv_index, doc_id_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[621, 1103, 1779, 5362, 7759]"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_bool_search('мем | жопа', inv_index, doc_id_set)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[621, 1103, 1779, 5362, 7759]"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_bool_search('мем | жопа', inv_index, doc_id_list)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'h\\x87'"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_tree_compressed(query, NodeType):\n",
    "    \"\"\"\n",
    "    return: leafs, tree\n",
    "    \"\"\"\n",
    "    tree = dict()\n",
    "    last_node_key = 0\n",
    "    raw_tokens = tokenize(query)\n",
    "    tokens = []\n",
    "    for i, token in enumerate(raw_tokens):\n",
    "        if token not in ['&', '|', '!', '(', ')']:\n",
    "            token = '_'.join( (token, str(i)) )\n",
    "        tokens.append(token)\n",
    "    \n",
    "    tokens = rpn( tokens )\n",
    "    \n",
    "    stack = []\n",
    "    leafs = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == '&' or token == '|':\n",
    "            tree[last_node_key] = NodeType(2, token)\n",
    "            node_id_right = stack.pop()\n",
    "            node_id_left = stack.pop()\n",
    "            tree[node_id_left].parent = last_node_key\n",
    "            tree[node_id_right].parent = last_node_key\n",
    "            tree[last_node_key].left_ind = node_id_left\n",
    "            tree[last_node_key].right_ind = node_id_right\n",
    "            stack.append(last_node_key)\n",
    "            \n",
    "        elif token == '!':\n",
    "            tree[last_node_key] = NodeType(3, token)\n",
    "            node_id_left = stack.pop()\n",
    "            tree[node_id_left].parent = last_node_key\n",
    "            tree[last_node_key].left_ind = node_id_left\n",
    "            stack.append(last_node_key)\n",
    "            \n",
    "        else:\n",
    "            tree[last_node_key] = NodeType(1, token)\n",
    "            stack.append(last_node_key)\n",
    "            leafs.append(last_node_key)\n",
    "        \n",
    "        last_node_key += 1\n",
    "    \n",
    "    if len(stack) != 1: # stack[-1] - root\n",
    "        return None, tree\n",
    "        \n",
    "    return stack[0], tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompressedFlowNode:\n",
    "    def __init__(self, node_type, val=None, left_ind=-1, right_ind=-1):\n",
    "        \"\"\"\n",
    "        node_type:\n",
    "            1 - leaf - one query string\n",
    "            2 - not leaf - &|\n",
    "            3 - not leaf - ! - use only left child\n",
    "        \n",
    "        \"\"\"\n",
    "        self.type = node_type\n",
    "        self.left_ind = left_ind\n",
    "        self.right_ind = right_ind\n",
    "        self.parent = -1\n",
    "        self.val = val\n",
    "        \n",
    "        self.is_matched = False \n",
    "        self.acceptable_doc_id = -1\n",
    "        if node_type == 1:\n",
    "            self.list_doc_id = -1 ### new\n",
    "    \n",
    "\n",
    "    def set_state(self, tree, struct_inv_index, fantom_list, cur_doc_id):\n",
    "        if self.type == 1:\n",
    "            #print(self.list_doc_id)\n",
    "            #print('has:' , struct_inv_index.has(self.val), self.val)\n",
    "            if not struct_inv_index.has(self.val):\n",
    "                print('ALARM!')\n",
    "                self.acceptable_doc_id = MAX_DOC_ID\n",
    "                self.is_matched = False\n",
    "            else:\n",
    "                while self.list_doc_id < 0 or self.list_doc_id < cur_doc_id:\n",
    "                    self.list_doc_id = struct_inv_index.update_step(self.val)\n",
    "                        \n",
    "                #print( 'cur: ', fantom_list.list_doc_id )\n",
    "                fantom_list.update(cur_doc_id)\n",
    "                #print( 'next: ', fantom_list.list_doc_id )\n",
    "                self.acceptable_doc_id = self.list_doc_id\n",
    "                #print( 'self.acceptable_doc_id: ', self.acceptable_doc_id, self.val)\n",
    "                self.is_matched = (self.acceptable_doc_id == cur_doc_id)\n",
    "            \n",
    "        elif self.val == '!':\n",
    "            if tree[self.left_ind].is_matched:\n",
    "                self.is_matched = False\n",
    "                self.acceptable_doc_id = fantom_list.get_next_id() #doc_id_list[fantom_list_pos + 1]\n",
    "            else:\n",
    "                if tree[self.left_ind].acceptable_doc_id == fantom_list.get_current_id():\n",
    "                                                        # doc_id_list[fantom_list_pos]:\n",
    "                    self.is_matched = False\n",
    "                    self.acceptable_doc_id = fantom_list.get_next_id() # doc_id_list[fantom_list_pos + 1] \n",
    "                else:\n",
    "                    self.is_matched = True\n",
    "                    self.acceptable_doc_id = fantom_list.get_current_id() # doc_id_list[fantom_list_pos]\n",
    "                    \n",
    "        elif self.val == '&':\n",
    "            self.acceptable_doc_id = max(tree[self.left_ind].acceptable_doc_id,\n",
    "                                        tree[self.right_ind].acceptable_doc_id)\n",
    "            self.is_matched = tree[self.left_ind].is_matched & tree[self.right_ind].is_matched\n",
    "            \n",
    "        elif self.val == '|':\n",
    "            self.acceptable_doc_id = min(tree[self.left_ind].acceptable_doc_id,\n",
    "                                        tree[self.right_ind].acceptable_doc_id)\n",
    "            self.is_matched = tree[self.left_ind].is_matched | tree[self.right_ind].is_matched\n",
    "        else:\n",
    "            print('Unknown bool operation', file=sys.stderr)\n",
    "            \n",
    "            \n",
    "class CompressedFantomList:\n",
    "    def __init__(self, list_fname='_id_list'):\n",
    "        self.list_pos = -1\n",
    "        self.doc_id_iter = VarbyteDecoder().decode(list_fname)\n",
    "        self.list_doc_id = None\n",
    "        \n",
    "    def update(self, cur_doc_id):\n",
    "        if self.list_doc_id is None:\n",
    "            self.list_doc_id = next(self.doc_id_iter)\n",
    "        \n",
    "        while self.list_doc_id < cur_doc_id:\n",
    "            self.list_doc_id = next(self.doc_id_iter, MAX_DOC_ID)\n",
    "            \n",
    "    def get_current_id(self):\n",
    "        return self.list_doc_id\n",
    "    def get_next_id(self):\n",
    "        self.list_doc_id = next(self.doc_id_iter, MAX_DOC_ID)\n",
    "        return self.list_doc_id\n",
    "    \n",
    "\n",
    "class InvIndex:\n",
    "    def __init__(self):\n",
    "        self.inv_index = dict()\n",
    "    def reset(self, keys):\n",
    "        self.keys = set(keys)\n",
    "        for i, token in enumerate(keys):\n",
    "            raw_key = token.split('_')[0]\n",
    "            self.inv_index[token] = VarbyteDecoder().decode(raw_key)\n",
    "        \n",
    "    def has(self, val):\n",
    "        return val in self.keys\n",
    "    def update_step(self, key):\n",
    "        return next(self.inv_index[key]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compressed_flow_bool_search(query, inv_index_keys):\n",
    "    \n",
    "    root, tree = get_query_tree_compressed(query, CompressedFlowNode)\n",
    "    if root is None or len(tree) == 0:\n",
    "        return []\n",
    "    \n",
    "    raw_tokens = tokenize(query)\n",
    "    tokens = []\n",
    "    for i, token in enumerate(raw_tokens):\n",
    "        if token not in ['&', '|', '!', '(', ')']:\n",
    "            tokens.append( '_'.join( (token, str(i)) ))\n",
    "            \n",
    "    print(tokens)\n",
    "    print(tree[root].val)\n",
    "    \n",
    "    compr_inv_index = InvIndex()\n",
    "    compr_inv_index.reset( tokens )\n",
    "    posting_list = []\n",
    "    cur_doc_id = -1\n",
    "    \n",
    "    fantom_list = CompressedFantomList()\n",
    "    \n",
    "    while cur_doc_id != MAX_DOC_ID:\n",
    "        \n",
    "        stack = [root]\n",
    "        used = [False] * len(tree)\n",
    "        used[root] = True\n",
    "        \n",
    "        while len(stack) != 0:\n",
    "            if tree[stack[-1]].left_ind != -1 and not used[tree[stack[-1]].left_ind]:\n",
    "                stack.append(tree[stack[-1]].left_ind)\n",
    "                used[stack[-1]] = True\n",
    "            elif tree[stack[-1]].right_ind != -1 and not used[tree[stack[-1]].right_ind]:\n",
    "                stack.append(tree[stack[-1]].right_ind)\n",
    "                used[stack[-1]] = True\n",
    "            else:\n",
    "                tree[stack.pop()].set_state(tree, compr_inv_index, fantom_list, cur_doc_id)\n",
    "        \n",
    "        if tree[root].acceptable_doc_id == cur_doc_id:\n",
    "            posting_list.append(cur_doc_id)\n",
    "            cur_doc_id += 1\n",
    "        else:\n",
    "            cur_doc_id = tree[root].acceptable_doc_id\n",
    "\n",
    "    return posting_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1391, 2446, 3078, 3441, 4316, 5136, 5345, 5446, 5656, 6553, 6640, 6746, 6885, 7108, 7136, 7221, 7460, 7834, 7982, 8403, 8423, 8906, 9210, 9537, 9722]\n"
     ]
    }
   ],
   "source": [
    "print( flow_bool_search('собаки', inv_index, doc_id_list) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48, 190, 1419, 1854, 2179, 2275, 2446, 4129, 5391, 5446, 6746, 9605, 9694]\n"
     ]
    }
   ],
   "source": [
    "print( flow_bool_search('кошки', inv_index, doc_id_list) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['медведев_1', 'кошки_4', 'собаки_6', 'путин_10', 'медведев_12']\n",
      "|\n",
      "[48, 190, 1391, 1419, 1854, 2179, 2275, 2446, 3078, 3441, 4316, 5345, 5391, 5446, 5656, 6553, 6640, 6746, 6885, 7108, 7136, 7221, 7460, 7834, 7982, 8403, 8423, 8906, 9210, 9537, 9605, 9694, 9722]\n"
     ]
    }
   ],
   "source": [
    "print( compressed_flow_bool_search('!медведев & (кошки | собаки) | !путин & медведев', list(inv_index.keys())) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2446, 5446, 6746]\n"
     ]
    }
   ],
   "source": [
    "print( flow_bool_search('собаки & кошки', inv_index, doc_id_list) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1779, 4118, 9605]"
      ]
     },
     "execution_count": 782,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( simple_bool_search('кошка', inv_index, doc_id_set) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.data_len = self.buffer_size\n",
    "        self.pos = -1\n",
    "        \n",
    "    def create(self, token):\n",
    "        self.fin = open('/'.join(('index', token)), 'rb')\n",
    "    \n",
    "    def get_byte(self):\n",
    "        if self.pos == -1 or self.pos == self.data_len:\n",
    "            self.data = self.fin.read(self.buffer_size)\n",
    "            self.data_len = len(self.data)\n",
    "            self.pos = 0\n",
    "            if self.data_len == 0:\n",
    "                print('End of file reached', file=sys.stderr)\n",
    "                return None\n",
    "        \n",
    "        self.pos += 1\n",
    "        return self.data[self.pos - 1:self.pos]\n",
    "\n",
    "\n",
    "class VarbyteDecoder:\n",
    "    def __init__(self, buffer_size=128): \n",
    "        self.buffer = Buffer(buffer_size)\n",
    "    \n",
    "    def _decode_digit(self, fin):\n",
    "        num = 0\n",
    "        mlt = 1\n",
    "        while True:\n",
    "            part = struct.unpack('B', self.buffer.get_byte())[0]\n",
    "            if part >= 128:\n",
    "                num = num + mlt * (part - 128)\n",
    "                break\n",
    "            num = num + mlt * part\n",
    "            mlt *= 128\n",
    "        return num\n",
    "    \n",
    "    \n",
    "    def decode(self, token):\n",
    "        with open('/'.join(('index', token)), 'rb') as fin:\n",
    "            self.buffer.create(token)\n",
    "            start = self._decode_digit(fin)\n",
    "            while True:\n",
    "                yield start\n",
    "                num = self._decode_digit(fin)\n",
    "                if num == 0:\n",
    "                    break\n",
    "                start += num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = VarbyteDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161, 622, 1941, 2655, 3078, 3173, 3273, 4278, 4924, 5354, 5625, 5819, 6322, 7136, 8403, 8951, 9189, 9210, 9641, 1000000000]\n"
     ]
    }
   ],
   "source": [
    "print([s for s in dec.decode('собака')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7759, 1000000000]\n",
      "[1779, 4118, 9605, 1000000000]\n",
      "[161, 622, 1941, 2655, 3078, 3173, 3273, 4278, 4924, 5354, 5625, 5819, 6322, 7136, 8403, 8951, 9189, 9210, 9641, 1000000000]\n"
     ]
    }
   ],
   "source": [
    "print(inv_index['жопа'])\n",
    "print(inv_index['кошка'])\n",
    "print(inv_index['собака'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp.pkl', 'wb') as f:\n",
    "    pickle.dump({'a' : [1,2,4], 'b' : [3,6,1], 'c': [1,1,1]}, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': [1, 2, 4], 'b': [3, 6, 1], 'c': [1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "with open('tmp.pkl', 'rb') as f:\n",
    "    print(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
